[{"content":"Mateo Díaz\\\\(^2\\) # I will join the Department of Applied Mathematics and Statistics at Johns Hopkins University in the Fall of 2023 as an Assistant Professor. If you are planning to apply to our Ph.D. program and would like to work with me, feel free to include my name in your statement of purpose. About #I am Postdoctoral Scholar at Caltech hosted by Venkat Chandrasekaran and Joel Tropp. I obtained my PhD in Applied Mathematics from Cornell University advised by Damek Davis. Before Cornell, I completed a MSc in Mathematics and two BS in Mathematics, and Systems and Computing Engineering at Universidad de los Andes. There I was co-advised by Mauricio Junca and Mauricio Velasco. I spent the Fall of 2020 with the Algorithms and Optimization team at Google Research, hosted by Miles Lubin and David Applegate.\nResearch interests #I am interested in the beautiful interplay between continuous optimization, geometry, and statistics and its applications to data science, machine learning and signal processing.\nContact #email: \u0026lt; first_name \u0026gt; dd \u0026lt; at \u0026gt; caltech \u0026lt; dot \u0026gt; edu office: Annenberg 305 mail: 1200 E. California Blvd., Mail Code 305-16, Pasadena, CA 91125 Publications #In the pipeline # Stochastic approximation with decision-dependent distributions: asymptotic normality and optimality (with J. Cutler and Drusvyatskiy) Submitted, 2022. PDF\nClustering a Mixture of Gaussians with Unknown Covariance (with D. Davis and K. Wang) Submitted, 2021. PDF\nInfeasibility detection with primal-dual hybrid gradient for large-scale linear programming (with D. Applegate, H. Lu, and M. Lubin) Submitted, 2021. PDF\nPublished # Optimal Convergence Rates for the Proximal Bundle Method (with B. Grimmer) SIAM Journal on Optimization, 2022. PDF\nEscaping strict saddle points of the Moreau envelope in nonsmooth optimization (with D. Davis and D. Drusvyatskiy) SIAM Journal on Optimization, 2022. PDF\nOptimization of vaccination for COVID-19 in the midst of a pandemic (with Q. Luo, R. Weightman, S. T. McQuade, E. Trélat, W. Barbour, D. Work, S. Samanaranayake, B. Piccoli) Networks and Heterogeneous Media, 2022. PDF\nPractical Large-Scale Linear Programming using Primal-Dual Hybrid Gradient (with D. Applegate, O. Hinder, H. Lu, M. Lubin, B. O\u0026rsquo;Donoghue, and W. Schudy) NeurIPS, 2021. PDF\nLow-rank matrix recovery with composite optimization: good conditioning and rapid convergence (with V. Charisopoulos, Y. Chen, D. Davis, L. Ding, D. Drusvyatskiy) Foundations of Computational Mathematics, 2021. PDF\nEfficient Clustering for Stretched Mixtures: Landscape and Optimality (with K. Wang and Y. Yan) NeurIPS, 2020. PDF\nComposite optimization for robust rank one bilinear sensing (with V. Charisopoulos, D. Davis, and D. Drusvyatskiy) Information and Inference, 2020. PDF\nLocal angles and dimension estimation from data on manifolds (with A. Quiroz, M. Velasco) Journal of Multivariate Analysis, 2019. PDF\nThe nonsmooth landscape of blind deconvolution Workshop on Optimization for Machine Learning, 2019. PDF\nCompressed sensing of data with known distribution (with M. Junca, F. Rincón and M. Velasco) Applied and Computational Harmonic Analysis, 2018. PDF\nIn Search of Balance: The Challenge of Generating Balanced Latin Rectangles (with C. Gomes, R. Le Bras) CPAIOR 2017. PDF\nTeaching #Cornell # ORIE 6340: Mathematics of Data Science Spring 2021 (Teaching Assistant).\nORIE 5270: Big Data Technologies Spring 2020 (Instructor).\nORIE 6125: Computational Methods in Operations Research Spring 2020 (Instructor).\nORIE 3300: Optimization I Summer 2017 (Teaching Assistant).\nUniandes # MATE-2604 Numerical Analysis Spring 2015 (Lecturer).\nMATE-1201 Precalculus Fall 2015 (Lecturer).\nMATE-1105 Linear Algebra Fall 2012, Spring 2013, Fall 2014 (Lecturer).\nRandom photos #Here are some pictures of places where I have lived.\nLeticia, Amazonas, Colombia\nSunset at the Amazon river - December 2018\nBogotá, Colombia\nUniandes - December 2017\nUpstate New York\nBiking near Ithaca - September 2018\nWatkins Glen in the winter - January 2021\nWatkins Glen in the summer - June 2021\nLos Angeles\nSunset behind the Hollywood sign - November 2021\n","date":null,"permalink":"/posts/all-contents/","section":"Posts","summary":"","title":""},{"content":"Mateo Díaz received his PhD in Applied Mathematics from Cornell University in 2021, followed by a postdoctoral appointment in Computing and Mathemtical Sciences at Caltech, 2021-2023. He joined the Department of Applied Mathematics and Statistics at Johns Hopkins University in 2023. Mateo\u0026rsquo;s work broadly revolves on continuous optimization, statistics, and geometry and its applications to signal processing, data science and machine learning. Mateo has received a number of awards, including the Beale\u0026mdash;Orchards-Hay Prize, NSF CAREER, and NSF Mathematical Foundations of AI 2025.\n","date":null,"permalink":"/bio/","section":"Mateo Díaz","summary":"","title":"Biosketch"},{"content":"Mateo Díaz received his PhD in Applied Mathematics from Cornell University in 2021, followed by a postdoctoral appointment in Computing and Mathemtical Sciences at Caltech, 2021-2023. He joined the Department of Applied Mathematics and Statistics at Johns Hopkins University in 2023. Mateo\u0026rsquo;s work focuses on the interplay between continuous optimization, statistics, and geometry and its applications to signal processing, data science and machine learning. He has received a number of awards, including the Beale\u0026mdash;Orchards-Hay Prize, NSF CAREER, and NSF Mathematical Foundations of AI.\n","date":null,"permalink":"/biosketch/","section":"Mateo Díaz","summary":"","title":"Biosketch"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"\u0026lt;first_name\u0026gt; dd caltech edu\n","date":null,"permalink":"/contact/","section":"Mateo Díaz","summary":"","title":"Contact"},{"content":" This website hosts the materials for a minicourse on concentration inequalities that I taught at the National University of Colombia in November of 2023. The course was delivered in Spanish. If you are interested in this subject, I strongly recommend the free video lectures on High-Dimensional Probability by, the great, Roman Vershynin. En este minicurso los asistentes tendrán la oportunidad de profundizar en el estudio de desigualdades de concentración clásicas y contemporáneas. A lo largo de tres sesiones, exploraremos desigualdades reconocidas, como las de Markov y Hoeffding, así como desigualdades más generales aplicables a variables aleatorias subgaussianas y subexponenciales. Además, se abordarán las versiones matriciales equivalentes de algunas de estas desigualdades. La teoría del curso será ilustrada con aplicaciones modernas, tales como el modelo de bloques estocásticos y reducción de dimensión, permitiendo así a los participantes adquirir habilidades aplicables en campos tan variados como la ciencia de datos, la teoría de redes y más. A través de una mezcla equilibrada de teoría y práctica, los estudiantes estarán preparados para utilizar estas poderosas herramientas en su trabajo e investigación futura, desarrollando un entendimiento sólido y aplicable de las desigualdades de concentración.\nEl curso está basado en el maravilloso libro de Roman Vershynin, \u0026ldquo;High-dimensional probability.\u0026rdquo; Pueden encontrar una copia gratuita aquí. Otras referencias muy buenas incluyen:\nBoucheron, Stéphane, Gábor Lugosi, and Olivier Bousquet. Concentration Inequalities (2013). Tropp, Joel A. Probability in High Dimensions. Lecture notes (Caltech) (Winter 2023). Van Handel, Ramon. Probability in High Dimension. Lecture Notes (Princeton University) (2014). Notas # Día 1: Desigualdades clásicas y distribuciones sub-gaussianas. Día 2: Matrices aleatorias Día 3: Concentración sin independencia. Ejercicios #Estos ejercicios están pensados para complementar los conceptos que cubrimos en clase. No se calificarán o corregirán, por favor no me manden las soluciones!\nProblema 1 #Demuestre que para todo \\(x \\in \\mathbb{R}\\) tenemos que \\(\\cosh(x) \\leq \\exp{x^2}\\).\nProblema 2 #Demuestre que la norma sub-gaussiana de una variable aleatoria \\(X\\) acotada por \\(1\\) está acotada por \\[ \\|X\\|_{\\psi_2} \\leq \\frac{1}{\\sqrt{\\log(2)}}. \\]\nProblema 3 #Considere la siguiente versión de la desigualdad de Hoeffding.\nTeorema. Sea \\(X_1, \\ldots, X_n\\) variables aleatorias independientes. Suponga que \\(X_i \\in [m_i, M_i]\\) para cada \\(i\\). Entonces, para cualquier \\(t \u0026gt; 0\\), tenemos \\[ \\mathbb{P}\\left(\\sum_{i=1}^{n} (X_i - \\mathbb{E} X_i) \\geq t\\right) \\leq \\exp\\left(-\\frac{2t^2}{\\sum_{i=1}^{N} (M_i - m_i)^2}\\right). \\]\nPruebe este resultado, posiblemente con alguna otra constante absoluta en lugar de 2 en la cola.\nProblema 4 #Sea \\(X\\) una variable aleatoria con esperanza cero. Demuestre que \\(X\\) es sub-gaussiana si y solo si exists una constant \\(K \u0026gt; 0\\) tal que para todo \\(\\lambda \\in \\mathbb{R}\\), \\[ \\mathbb{E}\\exp(\\lambda X) \\leq \\exp(K^2 \\lambda^2). \\]\nProblema 5 #Sea \\(X\\) una variable aleatoria sub-gaussiana. Defina \\[ \\|X\\|_{\\psi_2} = \\inf\\{K\u0026gt;0 \\colon \\mathbb{E}\\exp(X^2/K^2) \\leq 2\\}. \\] Demuestre que \\(\\|\\cdot\\|_{\\psi_2}\\) es una norma en el espacio de variables aleatorias sub-gaussianas.\nProblema 6 #Sea \\((\\mathcal{T}, d)\\) un espacio metrico y \\(K \\subseteq \\mathcal{T}\\) un subconjunto. Fije un \\(\\epsilon \u0026gt; 0\\), demuestre la siquiente desigualdad \\[ \\mathcal{P}(K, d, 2\\epsilon) \\leq \\mathcal{N}(K, d, \\epsilon) \\] donde \\(\\mathcal{P}(K, d, \\epsilon)\\) y \\(\\mathcal{N}(K, d, \\epsilon)\\) son el numero de empaquetamiento y de recubrimiento, respectivamente.\nProblema 7 #En nuestra definición de los números de cobertura de \\(K\\), requerimos que los centros \\( x_i \\) de las bolas \\( B(x_i, \\varepsilon) \\) que forman una cobertura estén en \\(K\\). Relajando esta condición, defina el número de cobertura exterior \\( N_{\\text{ext}}(K, d, \\varepsilon) \\) de manera similar pero sin requerir que \\( x_i \\in K \\). Demuestre que \\[ N_{\\text{ext}}(K, d, \\varepsilon) \\leq N(K, d, \\varepsilon) \\leq N_{\\text{ext}}(K, d, \\varepsilon/2). \\]\nProblema 8 #Dé un contraejemplo a la siguiente propiedad de monotonicidad: \\[ L \\subset K \\quad \\text{implica} \\quad N(L, d, \\varepsilon) \\leq N(K, d, \\varepsilon). \\] Demuestre una versión aproximada de monotonicidad: \\[ L \\subset K \\quad \\text{implica} \\quad N(L, d, \\varepsilon) \\leq N(K, d, \\varepsilon/2). \\]\nProblema 9 #Sea $A ∈ \\mathbb{R}n × m y $ ε ∈ (0, 1/2)$. Entonces para cualquier par de \\(\\epsilon\\)-mallas \\(\\mathcal{N} \\subseteq \\mathbb{S}^{m-1}\\) y \\(\\mathcal{M} \\subseteq \\mathbb{S}^{n-1}\\) tenemos que \\[ \\sup_{x \\in \\mathcal{N}, y \\in \\mathcal{M}} \\langle y, Ax \\rangle \\leq \\|A\\| \\leq \\frac{1}{1-2\\epsilon} \\sup_{x \\in \\mathcal{N}, y \\in \\mathcal{M}} \\langle y, Ax \\rangle. \\]\nProblema 10 #Recuerde el teorema de Davis-Kahan que vimos en clase:\nTeorema. Sean \\(S\\) y \\(T\\) matrices simetricas \\(n \\times n\\). Fije un indice \\(i \\in [n]\\) tal que \\[ \\min_{j \\colon j \\neq i} |\\lambda_i(S) - \\lambda_j(S)| = \\delta \u0026gt; 0. \\] Entonces, \\[ \\sin \\angle (v_i(S), v_i(T)) \\leq \\frac{2\\|S - T\\|}{\\delta}, \\] donde \\(v_i(X)\\) es el $i$-esimo vector propio de \\(X\\) y \\(\\angle(u,v)\\) denota el angulo entre \\(u\\) y \\(v\\) con valores entre \\([0, \\pi/2]\\).\nDemuestre que la conclusion del Teorema de Davis-Kahan que probamos en clase implica que existe \\(\\theta \\in \\{-1, 1\\}\\) tal que \\[ \\|v_i(S) - \\theta v_i(T)\\|_2 \\leq 2^{3/2}\\frac{\\|S - T\\|}{\\delta}. \\]\nProblema 11 #Sea \\(A\\) la matriz de adyacencia de un grafo aleatorio dado por el modelo de bloques estocastico \\(G(n, p, q)\\) con \\(n\\) par y \\(p \u0026gt; q\\). Suponga que ordenamos los nodos del grafo de forma que \\[D:= \\mathbb{E}(A) = \\begin{pmatrix} p \\mathbf{J} \u0026amp; q \\mathbf{J} \\\\ q \\mathbf{J} \u0026amp; p\\mathbf{J} \\end{pmatrix}\\] donde \\(\\mathbf{J} \\in \\{1\\}^{n/2 \\times n/2}\\) es la matriz de solo unos. Demuestre que esta matriz tiene rango dos y que sus valores propios y vectors propios son \\[ \\lambda_1 = \\left(\\frac{p+q}{2}\\right) n ,\\quad v_1 = \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\1 \\end{pmatrix}, \\quad \\lambda_2 = \\left(\\frac{p-q}{2}\\right) n, \\quad \\text{and} \\quad v_2 = \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\\\ -1 \\\\ \\vdots \\\\ -1 \\end{pmatrix}. \\]\nProblema 12 #Sea \\(X \\sim \\text{Unif}(\\mathbb{S}^{d-1})\\). Demuestre que \\(\\|\\sqrt(d)X_1\\|_{\\psi_2} \\leq C\\) para alguna constante \\(C \u0026gt; 0\\) independiente de la dimension.\nProblema 13 #Sea \\(X \\sim \\text{Unif}(\\mathbb{S}^{d-1})\\) y \\(f \\colon \\mathbb{S}^{d-1} \\rightarrow \\mathbb{R}\\) sea una funcion \\(L\\)-Lipschitz. Demuestre que \\[ \\|f(X) - \\mathbb{E }f(X)\\|_{\\psi_2} \\leq C\\frac{L}{\\sqrt{d}} \\] para alguna constante \\(C \u0026gt; 0\\) independiente de la dimension.\nExperimentos numericos #Estos son los experimentos que vimos en clase como motivacion.\nModelo de bloques estocasticos: Deteccion de comunidades #Primero generamos un grafo aleatorio y dibujamos las etiquetas reales.\nimport networkx as nx import matplotlib.pyplot as plt from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas import numpy as np from scipy.linalg import eigh # Generate a random Stochastic Block Model graph n = 100 p = 0.7 q = 0.01 sizes = [n // 2, n - n // 2] probs = [[p, q], [q, p]] G = nx.stochastic_block_model(sizes, probs) # Plot the graph with labels as colors fig, ax = plt.subplots() pos = nx.spring_layout(G) labels = [G.nodes[v][\u0026#39;block\u0026#39;] for v in G.nodes()] nx.draw(G, pos, node_color=labels, cmap=plt.cm.Paired, ax=ax) canvas = FigureCanvas(fig) filename = \u0026#34;true_labels.png\u0026#34; canvas.print_figure(filename) plt.close(fig) filename Ahora corremos el metodo espectral para estimar las etiquetas.\n# Compute the second eigenvector of the adjacency matrix A = nx.adjacency_matrix(G).toarray() w, v = eigh(A) fiedler_vector = v[:, -2] # Use the signs of the Fiedler vector as labels estimated_labels = np.sign(fiedler_vector) # Plot the graph with estimated labels fig2, ax2 = plt.subplots() nx.draw(G, pos, node_color=estimated_labels, cmap=plt.cm.Paired, ax=ax2) canvas2 = FigureCanvas(fig2) filename2 = \u0026#34;estimated_labels.png\u0026#34; canvas2.print_figure(filename2) plt.close(fig2) filename2 Finalmente, midamos el error.\ntrue_labels = np.array([1 if G.nodes[v][\u0026#39;block\u0026#39;] == 0 else -1 for v in G.nodes()]) error = min(np.sum(true_labels != estimated_labels), np.sum(true_labels != -estimated_labels)) print(\u0026#34;Misclassification error (up to sign):\u0026#34;, error) f\u0026#39;El error de clasificacion es {error}\u0026#39; El error de clasificacion es 0 Johnson-Lindenstrauss: Reduccion de dimension #\nimport numpy as np import matplotlib.pyplot as plt def random_orthogonal_projection(n, m): G = np.random.randn(n, m) Q, R = np.linalg.qr(G) return Q[:, :m] def random_projection(X, m): d = X.shape[1] P = random_orthogonal_projection(d, m) return np.sqrt(d / m) * X.dot(P) def norm_distortion(original, projected): original_norm = np.linalg.norm(original, axis=1) projected_norm = np.linalg.norm(projected, axis=1) distortion = np.abs(original_norm - projected_norm) / original_norm return np.max(distortion) def run_experiment(n, eps): d, D = 1000, 10000 k_values = np.arange(10, 3*int(np.log(n)/eps**2), 10) X_d = np.random.randn(n, d) X_D = np.random.randn(n, D) distortions_d = [norm_distortion(X_d, random_projection(X_d, k)) for k in k_values] distortions_D = [norm_distortion(X_D, random_projection(X_D, k)) for k in k_values] theoretical_k = 2 * np.log(n) / eps**2 plt.figure(figsize=(10, 6)) plt.plot(k_values, distortions_d, marker=\u0026#39;o\u0026#39;, label=f\u0026#34;Original dimension {d}\u0026#34;) plt.plot(k_values, distortions_D, marker=\u0026#39;x\u0026#39;, label=f\u0026#34;Original dimension {D}\u0026#34;) plt.axhline(eps, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;, label=f\u0026#39;epsilon = {eps}\u0026#39;) plt.axvline(theoretical_k, color=\u0026#39;g\u0026#39;, linestyle=\u0026#39;--\u0026#39;, label=f\u0026#39;2 log(n)/epsilon^2 ≈ {theoretical_k:.2f}\u0026#39;) plt.title(f\u0026#39;Distortion of the Norm vs Dimension of the Embedding (n={n})\u0026#39;) plt.xlabel(\u0026#39;Dimension of the Embedding (k)\u0026#39;) plt.ylabel(\u0026#39;Worst Distortion of the Norm\u0026#39;) plt.legend() plt.grid(True) image_file = f\u0026#39;jl-lemma-plot-n-{n}.png\u0026#39; plt.savefig(image_file) plt.close() return image_file # Plot for n = 100 image_file_100 = run_experiment(100, 0.25) image_file_100 Ahora con mas puntos.\nimage_file_2000 = run_experiment(2000, 0.25) image_file_2000 ","date":null,"permalink":"/desigualdades/","section":"Mateo Díaz","summary":"","title":"Desigualdades de concentración"},{"content":" I am co-organizing a Statistics and Data Science Workshop in Colombia (my home country). If you are a student interested in participating, please consider submitting a poster. About #I am an Assistant Professor in the Department of Applied Mathematics and Statistics and the Mathematical Institute for Data Science with a secondary appointment in the Department of Computer Science at Johns Hopkins University. Before that, I was a Postdoctoral Scholar at Caltech hosted by Venkat Chandrasekaran and Joel Tropp. I obtained my PhD in Applied Mathematics from Cornell University advised by Damek Davis. I completed a MSc in Mathematics and two BS in Mathematics, and in Systems and Computing Engineering at Universidad de los Andes where I was co-advised by Mauricio Junca and Mauricio Velasco.\nResearch interests #I am interested in the beautiful interplay between optimization, geometry, and statistics and its applications to data science, machine learning and signal processing.\nGroup #I\u0026rsquo;m lucky to advice and learn from:\nPedro Izquierdo Lehmann (AMS PhD, 2023 - ), Abdel Ghani Labassi (AMS PhD, 2024 - ), Daniel Lopez-Castaño (AMS PhD / Co-advised with Soledad Villar, 2024 - ) , Ian McPherson (AMS PhD / Co-advised with Mauro Maggioni, 2025 - ) Jonas Elmerraji (AMS DEng / Co-advised with James C. Spall, 2024 - ), Shengtai Yao (AMS MSE, 2025 - ) Haoruo Zhang (AMS Research Fellow, 2024 - ) Alumni # Fengsheng Lin (Data Science Masters, 2025), Yun You (Data Science Masters, 2024). If you are interested in working with me: I prioritize students who have taken a class with me and I know before hand. Additionally, please make sure to contact me well in advance; I prefer to have at least a year to develop a project together.\nContact #email: \u0026lt; first_name \u0026gt; dd \u0026lt; at \u0026gt; jhu \u0026lt; dot \u0026gt; edu office: Wyman S429 ","date":null,"permalink":"/","section":"Mateo Díaz","summary":"","title":"Mateo Díaz"},{"content":" This course revolves around a single question: how can we efficiently extract information from high-dimensional, noisy data with limited samples? This proof-driven graduate class develops the mathematical foundations behind modern data science, drawing on high-dimensional probability, optimization theory, and statistical learning. Core topics include concentration inequalities; dimension reduction; clustering; structure-exploiting inference; generalization via uniform convergence and algorithmic stability; and information-theoretic limits (Le Cam, Fano).\nCoordinates #Time: TTh 9:00PM - 10:15PM Location: Hodson 216\nPersonnel #Instructor:\nMateo Díaz (mateodd at jhu dot edu) OH Th 4:00PM - 5:30PM Wyman S429\nTeaching Assistants:\nPedro Izquierdo Lehmann (pizquie1 at jhu dot edu) OH M 9:30-10:15am Wyman S425\nIan McPherson (imcpher1 at jhu dot edu) OH F 9:00AM - 10:30AM Wyman S425\nLecture notes #Handwritten lecture notes will be posted here.\nLecture 1: Logistics and motivation Lecture 2: Chernoff method, sub-Gaussians, and Hoeffding\u0026rsquo;s inequality Lecture 3: Robust mean estimation and Chernoff\u0026rsquo;s inequality Lecture 4: Erdos-Renyi graphs, subexponentials, and Bernstein\u0026rsquo;s inequality Lecture 5: Johnson-Lindenstrauss Lemma and sub-Gaussian norm Lecture 6: Orlicz norms and McDiarmid\u0026rsquo;s inequality Lecture 7: Lipschitz functions of Gaussian random variables Lecture 8: Concentration of the norm of sub-Gaussian vectors Lecture 9: Matrix norms and the singular value decomposition Lecture 10: Introduction to perturbation theory Lecture 11: Davis-Kahan sin theorem Lecture 12: Proof of Davis-Kahan, Wedin\u0026rsquo;s theorem, and community detection Lecture 13: Nets, coverings, and packings Lecture 14: Covering numbers via volume and error correcting codes Lecture 15: Two-sided bounds on the singular values of sub-Gaussian matrices Lecture 16: Covariance estimation and clutering Gaussian Mixtures Lecture 17: Matrix Hoeffding Lecture 18: Maximum likelihood estimation Lecture 19: Ordinary least squares Lecture 20: Ridge least squares Lecture 21: Radamacher complexity Lecture 22: Vapnik-Chervonenkis dimension Lecture 23: Controlling the VC dimension Lecture 24: Dimension-free generalization bounds Textbook #We will use the following references:\n(Main reference) Roman Vershynin, High-Dimensional Probability: An Introduction with Applications in Data Science, 2nd Edition. Cambridge University Press (2025). Martin J. Wainwright, High-Dimensional Statistics: A Non-Asymptotic Viewpoint, 1st Edition. Cambridge University Press (2019). Francis Bach, Learning Theory from First Principles, 1st Edition. MIT Press (2024). Shai Shalev-Shwartz, Shai Ben-David, Understanding Machine Learning: From Theory to Algorithms, 1st Edition. Cambridge University Press (2014). Trevor Hastie, Robert Tibshirani, Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd Edition. Springer (2009). Terence Tao, Topics in Random Matrix Theory, 1st Edition. American Mathematical Society (2012). Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, Spectral Methods for Data Science: A Statistical Perspective, 1st Edition. Now Foundations and Trends (2021). Grading system #Your grade will take into account four components: Homework (50%), Take-home exam (20%), Final project (20%), and Participation (10%). In what follows we ellaborate on each of these components.\nHomework #Problem sets (approximately five) will be posted here and on the course Canvas. Some homework assignments include at least one question that involves the writing and testing of code; Python is prefered. Please submit homework assignments on Gradescope.\nGeneral policies. Your solutions must be written legibly and intelligibly in clear English. Use complete sentences. Points may be taken off for disorganized or illegible work. Collaboration is welcome, but your writeup must be your own\u0026mdash;do not copy answers from somebody else. Indicate at the top of your homework who you collaborated with on the assignment. If you believe your homework or exam grade to be in error, submit a regrade request. Every student will be allowed one late submission, up to 24 hours after the due date (no questions asked); just let us know that you will use your one shot.\nLarge language models policy. The use of large language models (LLMs) for brainstorming, coding assistance, and text polishing is permitted. What is not permitted is blindly answering your assignments or Midterm with LLMs. We reserve the right to run your submissions through https://gptzero.me/ and if it outputs a probability greater than 90% of being AI-generated, you may be required to take an oral examination to defend your work. Please begin your assignment with a short note explaining how, if at all, you used LLMs.\nHomework 1 Homework 2 Homework 3 Midterm #There will be one take-home exam with a date TBA. The exam will be posted on Canvas, and you will have two days to turn in your solutions through Gradescope. You may not discuss the exam with anyone or seek external help.\nFinal project #There will be a final project, which gives students the opportunity to explore topics related to the course that were not covered in class. Students may work individually or in groups of up to four, and the sole deliverable is a written report. Suggested topics will be released two weeks before the end of class, though students are welcome to propose their own with instructor approval. Reports should be written in the style of lecture notes if the topic is well established, or as a short research paper if the topic concerns current work. In either case, the report should state one main result or idea, explain why it is interesting and what implications it has (including potential new research directions), and present its proof—or a sketch thereof if the full argument is too long.\nParticipation #Participation weights 10% in the final grade. Engaging in class, Piazza, and office hours will count toward participation. This includes asking questions (even if you think they are silly!) and pointing out typos or mistakes.\n","date":null,"permalink":"/mds/","section":"Mateo Díaz","summary":"","title":"Mathematics of Data Science - 554.747 (Fall 2025)"},{"content":" This course introduces the mathematics of origami and origami design. It follows a flipped classroom format. Students complete assigned readings before each class, then participate in group discussions and problem-solving sessions during class time.\nCoordinates #Time: M 9:00AM - 10:00AM Location: Wyman N425\nInstructor #Mateo Díaz (mateodd at jhu dot edu)\nGrading system #Grades will be based on two components: participation (60%) and a final project (40%).\nParticipation #Students are expected to read before class and actively engage with the instructor and fellow students. Asking questions, suggesting potential solutions, and sharing thoughts during class are ways to engage.\nFinal project #For the final project, students have three possibilities: (1) read a paper concerning Origami or a chapter from [O] (discuss with the instructor before choosing) and present it to class, (2) design an arbitrary origami figure and do a presentation about the design process and the decisions that it involved, (3) solve one of the challenges below and do a presentation about it.\nNeedless to say if you choose to do (2) or (3) you need to bring a physical folded figure to class.\nReadings #We will use two books:\nOrigami Design Secrets: Mathematical Methods for an Ancient Art by Robert Lang (Second Edition) [ODS]. Origametry: Mathematical Methods in Paper Folding by Thomas Hull [O]. A copy of [ODS] will be provided to each student (to be returned by the end of the semester). [O] can be accessed online by Hopkins students in this link.\nDate Topic Reading September 9 Introduction - September 16 Basics: diagrams, crease patterns, and traditional basis ODS: 2, 4 September 23 Origami constructions O: 1.1, 1.2, 1.3 September 30 Axioms and solvability of equations O: 1.5, 2.1, 2.2 October 7 Splitting points and grafting ODS: 5, 6 October 14 Circle Packing ODS: 9 October 21 Molecules ODS: 10 October 28 Tree theory ODS: 11 November 4 Box pleating ODS: 12 November 11 When can a crease pattern be folded flat? O: 5.1, 5.2, 5.3, or This paper November 18 Project discussion - November 25 THANKSGIVING - December 2 Final project presentations - Challenges #There exist many solutions for some of these challenges. Do not look them up, do it yourself!\nDesign a Callinectes sapidus (Maryland blue crab) using a one square pice of paper. Design a Icterus galbula (Baltimore oriole) using one square piece of paper. Your figure must incorporate a \u0026ldquo;color change\u0026rdquo; to show the oriole\u0026rsquo;s distinctive orange/black patterns. Use paper that\u0026rsquo;s orange on one side and black on the other. The final design should display both colors to match the oriole\u0026rsquo;s appearance. Design a Cyanocitta cristata (Blue jay) using one square piece of paper. Just as with the previous challenge, your figure must incorporate a \u0026ldquo;color change\u0026rdquo; (white and blue). Design a \u0026ldquo;3D shadow\u0026rdquo; of a 4D hypercube (you can use modular origami). Design an origami modular for three intersecting golden rectangles. Design an origami tesselation (one square pice of paper) of at least two regular tessellations of the plane (one has to use hexagons). Design origami modulars for at least two of the Platonic solids. Design an origami modular of a polyhedral stellation. Resources # Useful websites CFC Origami Gilad\u0026rsquo;s Origami Page Origami Shop What paper to use? Simple figures: any paper. Intermediate figures: Tissue foil paper for beginners. Complex figures: Double tissue paper (with methyl cellulose). You can make the later two papers. All of the materials to make these can be found in Blick Art Materials.\n","date":null,"permalink":"/origami/","section":"Mateo Díaz","summary":"","title":"Mathematics of Origami (Fall 2024)"},{"content":" This course considers algorithms for solving various important nonlinear optimization problems and, in parallel, develops the supporting theory. Our primary focus will be on unconstrained optimization. Topics will include: necessary and sufficient optimality conditions; first-order methods, e.g., (sub)gradient descent, for smooth and nonsmooth optimization, second-order methods, e.g., Newton, quasi-Newton and trust-region methods; stochastic gradient and coordinate descent methods; linear and nonlinear least squares problems and conjugate gradient methods. If time permits we will cover: linear programming, minimax optimization, composite optimization.\nCoordinates #Time: TTh 4:30PM - 5:45PM Location: Maryland 109\nPersonnel #Instructor:\nMateo Díaz (mateodd at jhu dot edu) OH M 4:00PM - 6:00PM Wyman S429\nTeaching Assistants:\nKaleigh Rudge (krudge2 at jhu dot edu) OH W 10:00AM - 11:20AM Wyman S425\nThabo Samakhoana (tsamakh1 at jhu dot edu)\nOH Th 10:00AM - 11:20AM Wyman S425\nRoy Siegelmann (rsiege15 at jh dot edu) OH T 7:00PM - 8:20PM Wyman S425\nSyllabus #The syllabus can be found here.\nLecture notes # Lecture 1: Introduction Lecture 2: Minima and first-order optimality conditions Lecture 3: Second-order optimality conditions and basic convexity Lecture 4: More convexity, geometry of smooth convex functions, and subdifferentials Lecture 5: Gradient descent, descent lemma, and stepsizes Lecture 6: Guarantees for nonconvex smooth optimization Lecture 7: Better guarantees for convex, and strongly convex functions Lecture 8: Accelerated gradient descent Lecture 9: Lower bounds Lecture 10: Proximal operator and Forward-backward method Lecture 11: Guarantees for the Forward-backward method Lecture 12: Acceleration and Alternating projections Lecture 13: Subgradient descent Lecture 14: Stochastic gradient descent Lecture 15: SGD guarantees Lecture 16: Newton-Raphson method Lecture 17: Newton-Raphson local quadratic convergence and its issues Lecture 18: Modified Newton methods Lecture 19: Quasi-Newton methods Lecture 20: BFGS and DFP Lecture 21: Convergence guarantees for BFGS Lecture 22: Conjugate gradient method Lecture 23: CG guarantees and Nonlinear Least Squares Lecture 24: Trust Region Methods Lecture 25: Trust Region Methods continued Lecture 26: Weakly convex functions and composite optimization Homework #Homework assignments (approximately five) will be posted here and on the course Canvas. Most homework assignments include at least one question that involves the writing and testing of code. Python is prefered, and I will use it in my demos.\nHomework 0: This is an optional homework, just to help you learn/practice python. Do not submit it. Homework 1. Homework 2. Homework 3. Homework 4. Homework 5. Textbook #We will not be following any particular textbook. Notes will be posted on Canvas and the website. Other potentially useful textbooks as references (but not required):\nJ. Nocedal and S. Wright, Numerical Optimization, Second Edition, Springer, (2006), Y. Nesterov, Introductory Lectures on Convex Optimization: A Basic Course, Kluwer Academic Publishers, Norwell, MA, (2004), A. Ruszcynski, Nonlinear Optimization, Princeton University Press, Princeton, NJ (2006), D. P. Bertsekas, Nonlinear Programming, Second Edition, Athena Scientific, Belmont, MA, (1999). D. Drusvyatskiy, Convex Analysis and Nonsmooth Optimization, Lecture notes, (2020) PDF. Grading system #We will use an ingenious grading scheme invented by Ben Grimmer for one of the previous iterations of this class. Course grades will be based on four components: Homework, Midterm, Final, Participation. These are described individually below. I will maximize the course score that I give each of you. To optimize each student\u0026rsquo;s course score, I will solve the following optimization problem.\nDenote the student’s performance in each of these four components as\n\\[ \\begin{array}{rl} C_H \u0026amp;= \\text{Homework score},\\\\ C_M \u0026amp;= \\text{Midterm score},\\\\ C_F \u0026amp;= \\text{Final score, and}\\\\ C_P \u0026amp;= \\text{Participation score}. \\end{array}\\] All of the above are scores out of \\(100\\). The optimization problem will be solved individually for each student to give them the best rubric and highest course score I can justify. Then, a rubric for grading this student is given by selecting weights for these four components as \\[ \\begin{array}{rl} H \u0026amp;= \\text{Homework weight}, \\\\ M \u0026amp;= \\text{Midterm exam weight}, \\\\ F \u0026amp;= \\text{Final exam weight}, \\\\ P \u0026amp;= 100 - H - M - F. \\end{array}\\] Notice the participation weight is determined by the other three since they must sum to 100. Each student’s score is given by maximizing over the set of all reasonable rubrics \\((H, M, F)\\) by solving \\[\\begin{array}{rl} \\text{max} \u0026amp; C_H H + C_M M + C_F F + C_P (100 - H - M - F)\u0026amp;\u0026amp;\\\\ \\text{subject to} \u0026amp; \\\\ \u0026amp; \\begin{array}{rllll} \u0026amp;\u0026amp; (H, M, F) \u0026amp;\\in \\mathbb{R}^3\u0026amp;\\\\ \u0026amp;\u0026amp; H + M + F \u0026amp;\\leq 100 \u0026amp;\\text{(Percentages are at most 100)}\\\\ 15 \u0026amp;\\leq\u0026amp; H, M \u0026amp; \u0026amp;\\text{(Homework and Midterm matter)}\\\\ M \u0026amp;\\leq\u0026amp; F \u0026amp;\u0026amp;\\text{(Final is more important than Midterm)}\\\\ 50 \u0026amp; \\leq\u0026amp; M + F \u0026amp; \\leq 80 \u0026amp; \\text{(Exams are most, but not all of the score)}\\\\ 90 \u0026amp; \\leq\u0026amp; H + M + F \u0026amp;\u0026amp; \\text{(H, M, and F are the majority of the score)}. \\end{array} \\end{array}\\]\nMidterm and Final #Additional assessment will be based on one midterm exam and the final exam, both will be take home. The dates of the midterm and final exams will be posted on the course Canvas as they become available, although the date of the final exam is determined by the official JHU final exam schedule. The Midterm will be made available on Canvas on Friday October 13, 2023 and will be due at the beginning of class on Tuesday, October 17, 2023. The Final Exam will be made available on Canvas on Wednesday, December 13 and due by midnight on Friday, December 15.\nParticipation #The grading program described above will assign between 0 and 10 percent of the course to be a participation grade. As a result, you can get full marks in the course without any participation. Students will receive full points in participation for doing any of: scribing lectures, engaging during lectures, engaging in office hours, or asking insightful questions.\nScribing #Scribing at least one lecture is strongly encouraged. As mentioned above, scribing counts towards participation. Given that we have more lectures than students, there might be at most two scribes per lecture. Please send me your notes within one week of the lecture. I expect the notes to be fairly polished, and I might ask to improve them before I upload them to the website. Whenever possible, please add figures that clarify concepts and ideas. I recommend using GeoGebra, or Grapher (if you use macOS). Also, we all make mistakes, so run the notes by a grammar checker, e.g., Microsoft Word or Grammarly.\nHere is a latex template for the lecture notes\n","date":null,"permalink":"/nonlinear-fa23/","section":"Mateo Díaz","summary":"","title":"Nonlinear Optimization I - 553.761 (Fall 2023)"},{"content":" This course considers algorithms for solving various important nonlinear optimization problems and, in parallel, develops the supporting theory. Our primary focus will be on unconstrained optimization. Topics will include: necessary and sufficient optimality conditions; first-order methods, e.g., (sub)gradient descent, for smooth and nonsmooth optimization, second-order methods, e.g., Newton, quasi-Newton and trust-region methods; stochastic gradient and coordinate descent methods; linear and nonlinear least squares problems and conjugate gradient methods. If time permits we will cover: linear programming, minimax optimization, composite optimization.\nCoordinates #Time: TTh 4:30PM - 5:45PM Location: Ames 234\nPersonnel #Instructor:\nMateo Díaz (mateodd at jhu dot edu) OH M 4:00PM - 5:30PM Wyman S429\nTeaching Assistants:\nPedro Izquierdo Lehmann (pizquie1 at jhu dot edu) OH Th 10:00AM - 11:30AM Wyman S425\nDaniel Lopez-Castaño (jlopezc1 at jhu dot edu) OH Tu 10:00AM - 11:30AM Wyman S425\nThabo Samakhoana (tsamakh1 at jhu dot edu)\nOH Wed 10:30AM - 11:15AM Wyman S425\nSyllabus #The syllabus can be found here.\nLecture notes #The lecture notes will be posted here.\nLecture 1: Introduction Lecture 2: Minima and first-order optimality conditions Lecture 3: Second-order optimality conditions and basic convexity Lecture 4: Geometry of smooth convex functions and subdifferentials Lecture 5: Gradient descent, descent lemma, and stepsizes Lecture 6: Guarantees for nonconvex smooth optimization Lecture 7: Better guarantees for convex, and strongly convex functions Lecture 8: Accelerated gradient descent Lecture 9: Lower bounds Lecture 10: Structured nonsmooth problems and the proximal operator Lecture 11: Guarantees for the Forward-backward method Lecture 12: Acceleration and Alternating projections Lecture 13: Subgradient descent Lecture 14: Stochastic gradient descent Lecture 15: SGD guarantees Lecture 16: Newton-Raphson method Lecture 17: Newton-Raphson local quadratic convergence and its issues Lecture 18: Modified Newton methods Lecture 19: Quasi-Newton methods Lecture 20: BFGS and DFP Lecture 21: Convergence guarantees for BFGS Lecture 22: Conjugate gradient method Lecture 23: CG guarantees and Nonlinear Least Squares Lecture 24: Trust Region Methods Lecture 25: Trust Region Methods continued Lecture 26: Weakly convex functions and composite optimization Homework #Homework assignments (approximately five) will be posted here and on the course Canvas. Most homework assignments include at least one question that involves the writing and testing of code. Python is prefered, and I will use it in my demos.\nHomework 1. Homework 2. Homework 3. Homework 4. Homework 5. Textbook #We will not be following any particular textbook. Notes will be posted on this website. Other potentially useful textbooks as references (but not required):\nJ. Nocedal and S. Wright, Numerical Optimization, Second Edition, Springer, (2006), Y. Nesterov, Introductory Lectures on Convex Optimization: A Basic Course, Kluwer Academic Publishers, Norwell, MA, (2004), A. Ruszcynski, Nonlinear Optimization, Princeton University Press, Princeton, NJ (2006), D. P. Bertsekas, Nonlinear Programming, Second Edition, Athena Scientific, Belmont, MA, (1999). D. Drusvyatskiy, Convex Analysis and Nonsmooth Optimization, Lecture notes, (2020) PDF. Grading system #We will use an ingenious grading scheme invented by Ben Grimmer for one of the previous iterations of this class. Course grades will be based on four components: Homework, Midterm, Final, Participation. These are described individually below. I will maximize the course score that I give each of you. To optimize each student\u0026rsquo;s course score, I will solve the following optimization problem.\nDenote the student’s performance in each of these four components as\n\\[ \\begin{array}{rl} C_H \u0026amp;= \\text{Homework score},\\\\ C_M \u0026amp;= \\text{Midterm score},\\\\ C_F \u0026amp;= \\text{Final score, and}\\\\ C_P \u0026amp;= \\text{Participation score}. \\end{array}\\] All of the above are scores out of \\(100\\). The optimization problem will be solved individually for each student to give them the best rubric and highest course score I can justify. Then, a rubric for grading this student is given by selecting weights for these four components as \\[ \\begin{array}{rl} H \u0026amp;= \\text{Homework weight}, \\\\ M \u0026amp;= \\text{Midterm exam weight}, \\\\ F \u0026amp;= \\text{Final exam weight}, \\\\ P \u0026amp;= 100 - H - M - F. \\end{array}\\] Notice the participation weight is determined by the other three since they must sum to 100. Each student’s score is given by maximizing over the set of all reasonable rubrics \\((H, M, F)\\) by solving \\[\\begin{array}{rl} \\text{max} \u0026amp; C_H H + C_M M + C_F F + C_P (100 - H - M - F)\u0026amp;\u0026amp;\\\\ \\text{subject to} \u0026amp; \\\\ \u0026amp; \\begin{array}{rllll} \u0026amp;\u0026amp; (H, M, F) \u0026amp;\\in \\mathbb{R}^3\u0026amp;\\\\ \u0026amp;\u0026amp; H + M + F \u0026amp;\\leq 100 \u0026amp;\\text{(Percentages are at most 100)}\\\\ 15 \u0026amp;\\leq\u0026amp; H, M \u0026amp; \u0026amp;\\text{(Homework and Midterm matter)}\\\\ M \u0026amp;\\leq\u0026amp; F \u0026amp;\u0026amp;\\text{(Final is more important than Midterm)}\\\\ 50 \u0026amp; \\leq\u0026amp; M + F \u0026amp; \\leq 80 \u0026amp; \\text{(Exams are most, but not all of the score)}\\\\ 90 \u0026amp; \\leq\u0026amp; H + M + F \u0026amp;\u0026amp; \\text{(H, M, and F are the majority of the score)}. \\end{array} \\end{array}\\]\nMidterm and Final #Additional assessment will be based on one midterm exam and the final exam, both will be take home. The dates of the midterm and final exams will be posted on the course Canvas as they become available, although the date of the final exam is determined by the official JHU final exam schedule.\nParticipation #The grading program described above will assign between 0 and 10 percent of the course to be a participation grade. As a result, you can get full marks in the course without any participation. Students will receive full points in participation for engaging during lectures, engaging in office hours, and/or asking insightful questions.\n","date":null,"permalink":"/nonlinear/","section":"Mateo Díaz","summary":"","title":"Nonlinear Optimization I - 553.761 (Fall 2024)"},{"content":" This course builds upon Nonlinear Optimization I to explore advanced topics in optimization theory and algorithms. Our primary focus will be on constrained optimization and duality theory. Topics include: convex analysis fundamentals and optimality conditions for constrained problems; Fenchel and Lagrangian duality theory; classical algorithmic solutions, such as, the simplex method and interior point methods, and modern splitting methods, e.g., ADMM and PDHG; and elements of variational analysis for handling nonsmooth and nonconvex problems.\nCoordinates #Time: MW 12:00M - 1:15PM Location: Bloomberg 278\nPersonnel #Instructor:\nMateo Díaz (mateodd at jhu dot edu) OH Th 3:00PM - 4:30PM Wyman S429\nTeaching Assistants:\nPedro Izquierdo Lehmann (pizquie1 at jhu dot edu) OH Tu 9:15-10:00am Wyman S425\nThabo Samakhoana (tsamakh1 at jhu dot edu)\nOH Fr 3:30-5:00pm Wyman S425\nLecture notes #The lecture notes will be posted here.\nLecture 1: Intro and motivation Lecture 2: Convex sets and separation Lecture 3: Convex functions and continuity Lecture 4: Subgradients, normals, and optimality conditions Lecture 5: Gordon\u0026rsquo;s theorem of alternatives and KKT conditions Lecture 6: Convex conjugate and Fenchel duality Lecture 7: Conic optimization Lecture 8: Subdifferential calculus, Lagrange duality, and Fenchel biconjugates Lecture 9: Extreme points of polyhedra and intro to Simplex Lecture 10: Full-blown Simplex Lecture 11: Intro to interior point methods Lecture 12: Revisiting Newton method Lecture 13: Polynomial time guarantee for a primal interior point method Lecture 14: Proximal point method Lecture 15: Krasnoselskii-Mann iteration Lecture 16: Maximal monotone operators and Augmented lagrangian method Lecture 17: Douglas Rachford Lecture 18: ADMM Lecture 19: PDHG Lecture 20: Frechet subdifferential Lecture 21: Fuzzy calculus Lecture 22: Limiting subdifferential Lecture 23: Clarke subdifferential Lecture 24: Ekeland\u0026rsquo;s variational principle and Inverse problems Lecture 25: Sufficient conditions for metric regularity Homework #Assignments (approximately five) will be posted here and on the course Canvas. Most homework assignments include at least one question that involves the writing and testing of code. Python is preferred, and I will use it in my demos.\nHomework 1. Homework 2. Homework 3. Homework 4. Homework 5. Textbook #We will not be following any particular textbook. Notes will be posted on this website. Other potentially useful textbooks as references (but not required):\nJ. Borwein and A. S. Lewis, Convex Analysis and Nonlinear Optimization, Springer, (2006), J. Nocedal and S. Wright, Numerical Optimization, Second Edition, Springer, (2006), R. T. Rockafellar, Convex Analysis, Princeton University Press (1970) J.-F. Bonnans and A. Shapiro, Perturbation Analysis of Optimization Problems, Springer (2000) R. T. Rockafellar and R. J-B. Wets, Variational Analysis, Springer (1998) H. H. Bauschke and P. L. Combettes, Convex Analysis and Monotone Operator Theory in Hilbert Spaces, Springer (2011) D. Drusvyatskiy, Convex Analysis and Nonsmooth Optimization, Lecture notes, (2020) PDF. Grading system #We will use an ingenious grading scheme invented by Ben Grimmer for one of the previous iterations of this class. Course grades will be based on four components: Homework, Midterm, Final, Participation. These are described individually below. I will maximize the course score that I give each of you. To optimize each student\u0026rsquo;s course score, I will solve the following optimization problem.\nDenote the student’s performance in each of these four components as\n\\[ \\begin{array}{rl} C_H \u0026amp;= \\text{Homework score},\\\\ C_M \u0026amp;= \\text{Midterm score},\\\\ C_F \u0026amp;= \\text{Final score, and}\\\\ C_P \u0026amp;= \\text{Participation score}. \\end{array}\\] All of the above are scores out of \\(100\\). The optimization problem will be solved individually for each student to give them the best rubric and highest course score I can justify. Then, a rubric for grading this student is given by selecting weights for these four components as \\[ \\begin{array}{rl} H \u0026amp;= \\text{Homework weight}, \\\\ M \u0026amp;= \\text{Midterm exam weight}, \\\\ F \u0026amp;= \\text{Final exam weight}, \\\\ P \u0026amp;= 100 - H - M - F. \\end{array}\\] Notice the participation weight is determined by the other three since they must sum to 100. Each student’s score is given by maximizing over the set of all reasonable rubrics \\((H, M, F)\\) by solving \\[\\begin{array}{rl} \\text{max} \u0026amp; C_H H + C_M M + C_F F + C_P (100 - H - M - F)\u0026amp;\u0026amp;\\\\ \\text{subject to} \u0026amp; \\\\ \u0026amp; \\begin{array}{rllll} \u0026amp;\u0026amp; (H, M, F) \u0026amp;\\in \\mathbb{R}^3\u0026amp;\\\\ \u0026amp;\u0026amp; H + M + F \u0026amp;\\leq 100 \u0026amp;\\text{(Percentages are at most 100)}\\\\ 15 \u0026amp;\\leq\u0026amp; H, M \u0026amp; \u0026amp;\\text{(Homework and Midterm matter)}\\\\ M \u0026amp;\\leq\u0026amp; F \u0026amp;\u0026amp;\\text{(Final is more important than Midterm)}\\\\ 50 \u0026amp; \\leq\u0026amp; M + F \u0026amp; \\leq 80 \u0026amp; \\text{(Exams are most, but not all of the score)}\\\\ 90 \u0026amp; \\leq\u0026amp; H + M + F \u0026amp;\u0026amp; \\text{(H, M, and F are the majority of the score)}. \\end{array} \\end{array}\\]\nMidterm and Final #Additional assessment will be based on one midterm exam and the final exam, both will be take home. The dates of the midterm and final exams will be posted on the course Canvas as they become available, although the date of the final exam is determined by the official JHU final exam schedule.\nParticipation #The grading program described above will assign between 0 and 10 percent of the course to be a participation grade. As a result, you can get full marks in the course without any participation. Students will receive full points in participation for engaging during lectures, engaging in office hours, and/or asking insightful questions.\n","date":null,"permalink":"/nonlinear2/","section":"Mateo Díaz","summary":"","title":"Nonlinear Optimization II - 553.763 (Spring 2025)"},{"content":" About # When? Where? What? Speakers #Keynote #Confirmed #Venue #Registration #Student Registration Here\nProgram #Travel grants #","date":null,"permalink":"/optiworkshop/","section":"Mateo Díaz","summary":"","title":"Optimization Workshop"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":" This course is an advanced graduate-level course designed to deepen students\u0026rsquo; understanding of the theoretical underpinnings of probability. Building on the foundational concepts introduced in Probability Theory I, this course delves into more complex topics such as martingales, Markov chains, Brownian motion, and stochastic calculus. Through rigorous mathematical proofs and problem-solving exercises, students will explore the interplay between theory and application, gaining insights into how probability theory informs various fields such as finance, physics, and statistics. The course aims to equip students with a solid theoretical background and analytical skills necessary for research and application in their respective disciplines.\nCoordinates #Time: MW 1:30PM - 2:45PM Location: Maryland 217\nPersonnel #Instructor:\nMateo Díaz (mateodd at jhu dot edu) OH M 3:00PM - 5:00PM Wyman S429 New OH W 3:00PM - 5:00PM Wyman S429\nTeaching Assistant:\nAo Sun (asun17 at jhu dot edu) OH F 9:00AM - 11:00AM Wyman S425\nSyllabus #The syllabus can be found here.\nLecture notes #Handwritten lecture notes will be posted here and on Canvas.\nLecture 1: Logistics and motivation Lecture 2: Characteristic functions and Levy\u0026rsquo;s inversion formula Lecture 3: Weak convergence and Levy\u0026rsquo;s Convergence Theorem Lecture 4: Poisson distribution and the Law of Rare Events Demo Lecture 5: Conditional Expectation Lecture 6: Properties of the Conditional Expectation Lecture 7: Martingales Lecture 8: Doob\u0026rsquo;s Optional Stopping and Convergence Theorems Lecture 9: Examples: Random Walks and Branching Process Lecture 10: Convergence in Lp Lecture 11: Uniform integrability and Convergence in L1 Lecture 12: Optional stopping via UI and Backwards martingales Lecture 13: Examples: Ballot Theorem, Strong Law of Large Numbers, and de Finetti\u0026rsquo;s Theorem Lecture 14: Markov Chains Lecture 15: Markov Property Lecture 16: Strong Markov Property Lecture 17: Recurrence and Transience Lecture 18: Stationary measures Lecture 19: Aperiodicity and convergence Lecture 20: Brownian motion Lecture 21: Existence of Brownian motion Lecture 22: The Markov Property for Brownian motion Lecture 23: The Strong Markov Property for Brownian motion Lecture 24: Brownian motion as a martingale Textbook #We will use the following references:\n(Main textbook) Rick Durrett, Probability: Theory and Examples, 5th edition. Cambridge University Press (2019). Available at https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf. Patrick Billingsley, Probability and Measure, Anniversary (or 3rd) edition. John Wiley \u0026amp; Sons (2012). Available online through JHU libraries: https://ebookcentral.proquest.com/lib/jhu/detail.action?docID=836625. Kai Lai Chung, A Course in Probability Theory. Academic Press (2001). David Williams, Probability with Martingales, 1st Edition. Cambridge Mathematical Textbooks (1991). Grading system #Your grade will take into account four components: Homework (40%), Take-home exam (25%), Final project (25%), and Participation (10%). In what follows we ellaborate on each of these components.\nHomework #Homework assignments (approximately five) will be posted here and on the course Canvas. Some homework assignments include at least one question that involves the writing and testing of code; Python is prefered. Please submit homework assignments on Gradescope.\nHomework 1 Homework 2 Homework 3 Homework 4 Homework 5 Take-home exam #There will be one take-home exam with a date TBA. The exam will be posted on Canvas, and you will have two days to turn in your solutions through Gradescope. You may not discuss the exam with anyone or otherwise seek external help.\nFinal project #There will be a final project; this is an opportunity for students to learn about related topics that we did not cover in class. Students can work in groups of two or three people. The final project includes two deliverables: a written report and a one-hour presentation. Topics for the final project will be released two weeks before the end of class (students are welcome to pick a different project provided the approval of the instructor). Each group can choose how to split the time among the group members. The group should aim to state one result/idea, place it in context (why is it interesting? What implications does it have? Does it open interesting new research directions?), and present the main idea of its proof.\nFinal project description\nParticipation #Participation weights 10% in the final grade. Engaging in class, Piazza, and office hours will count toward participation. This includes asking questions (even if you think they are silly!) and pointing out typos or mistakes.\n","date":null,"permalink":"/prob2/","section":"Mateo Díaz","summary":"","title":"Probability Theory II"},{"content":"Ph.D. Thesis # Complexity, conditioning, and saddle avoidance in nonsmooth optimization PhD Thesis, Cornell University, 2021. PDF In the pipeline # Preconditioned subgradient method for composite optimization: overparameterization and fast convergence (with L. Jiang, A. G. Labassi), 2025. PDF\nInvariant Kernels: Rank Stabilization and Generalization Across Dimensions (with D. Drusvyatskiy, J. Kendrick, R. R. Thomas), 2025. PDF\nPDLP: A Practical First-Order Method for Large-Scale Linear Programming (with D. Applegate, O. Hinder, H. Lu, M. Lubin, B. O\u0026rsquo;Donoghue, and W. Schudy), 2025. PDF This paper complements our 2021 conference paper.\nControlling the False Discovery Rate in Subspace Selection (with V. Chandrasekaran), 2024. PDF\nRobust, randomized preconditioning for kernel ridge regression (with E. N. Epperly, Z. Frangella, J. A. Tropp, R. J. Webber), 2023. PDF\nPublished # The radius of statistical efficiency (with J. Cutler and D. Drusvyatskiy) Foundations of Computational Mathematics, 2025. PDF\nBeyond Scores: Proximal Diffusion Models (with Z. Fang, S. Buchanan, J. Sulam) NeurIPS, 2025. PDF\nOn Transferring Transferability: Towards a Theory for Size Generalization (with E. Levin, Y. Ma, S. Villar) NeurIPS (Spotlight), 2025. PDF Clustering a Mixture of Gaussians with Unknown Covariance (with D. Davis and K. Wang) Bernoulli, 2025. PDF\nStochastic approximation with decision-dependent distributions: asymptotic normality and optimality (with J. Cutler and D. Drusvyatskiy) Journal of Machine Learning Research, 2024. PDF\nAny-dimensional equivariant neural networks (with E. Levin) AISTATS, 2024. PDF\nInfeasibility detection with primal-dual hybrid gradient for large-scale linear programming (with D. Applegate, H. Lu, and M. Lubin) SIAM Journal on Optimization, 2024. PDF\nOptimal Convergence Rates for the Proximal Bundle Method (with B. Grimmer) SIAM Journal on Optimization, 2023. PDF\nEscaping strict saddle points of the Moreau envelope in nonsmooth optimization (with D. Davis and D. Drusvyatskiy) SIAM Journal on Optimization, 2022. PDF\nOptimization of vaccination for COVID-19 in the midst of a pandemic (with Q. Luo, R. Weightman, S. T. McQuade, E. Trélat, W. Barbour, D. Work, S. Samanaranayake, B. Piccoli) Networks and Heterogeneous Media, 2022. PDF\nPractical Large-Scale Linear Programming using Primal-Dual Hybrid Gradient (with D. Applegate, O. Hinder, H. Lu, M. Lubin, B. O\u0026rsquo;Donoghue, and W. Schudy) NeurIPS, 2021. PDF\nLow-rank matrix recovery with composite optimization: good conditioning and rapid convergence (with V. Charisopoulos, Y. Chen, D. Davis, L. Ding, D. Drusvyatskiy) Foundations of Computational Mathematics, 2021. PDF\nEfficient Clustering for Stretched Mixtures: Landscape and Optimality (with K. Wang and Y. Yan) NeurIPS, 2020. PDF\nComposite optimization for robust rank one bilinear sensing (with V. Charisopoulos, D. Davis, and D. Drusvyatskiy) Information and Inference, 2020. PDF\nLocal angles and dimension estimation from data on manifolds (with A. Quiroz, M. Velasco) Journal of Multivariate Analysis, 2019. PDF\nThe nonsmooth landscape of blind deconvolution Workshop on Optimization for Machine Learning, 2019. PDF\nCompressed sensing of data with a known distribution (with M. Junca, F. Rincón and M. Velasco) Applied and Computational Harmonic Analysis, 2018. PDF\nIn Search of Balance: The Challenge of Generating Balanced Latin Rectangles (with C. Gomes, R. Le Bras) CPAIOR 2017. PDF\n","date":null,"permalink":"/publications/","section":"Mateo Díaz","summary":"","title":"Publications"},{"content":"Here are some pictures of places where I have lived (or visited).\n","date":null,"permalink":"/other/","section":"Mateo Díaz","summary":"","title":"Random photos"},{"content":"Here are some pictures of places where I have lived.\nLeticia, Amazonas, Colombia #Sunset at the Amazon river - December 2018\nBogotá, Colombia #Uniandes - December 2017\nUpstate New York #Biking near Ithaca - September 2018\nWatkins Glen in the winter - January 2021\nWatkins Glen in the summer - June 2021\nLos Angeles #Sunset behind the Hollywood sign - November 2021\nBaltimore #Group lunch at the Harbor - August 2025 ","date":null,"permalink":"/random/","section":"Mateo Díaz","summary":"","title":"Random photos"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Johns Hopkins # AMS 747: Mathematics of Data Science Fall 2025. AMS 762: Nonlinear Optimization 2 Spring 2025. AMS 761: Nonlinear Optimization 1 Fall 2024, Fall 2023. AMS 101: Freshman Experience in AMS: Mathematics of Origami Fall 2024. AMS 721: Probability Theory 2 Spring 2024. Cornell # ORIE 6340: Mathematics of Data Science Spring 2021 (Teaching Assistant).\nORIE 5270: Big Data Technologies Spring 2020.\nORIE 6125: Computational Methods in Operations Research Spring 2020.\nORIE 3300: Optimization I Summer 2017 (Teaching Assistant).\nUniandes # MATE-2604 Numerical Analysis Spring 2015 (Teaching Assistant).\nMATE-1201 Precalculus Fall 2015 (Teaching Assistant).\nMATE-1105 Linear Algebra Fall 2012, Spring 2013, Fall 2014 (Teaching Assistant).\nOther # Minicurso: Desigualdades de concentracion - Universidad Nacional de Colombia Noviembre 2023 (Instructor). ","date":null,"permalink":"/teaching/","section":"Mateo Díaz","summary":"","title":"Teaching"}]